# {{ template "chart.header" . }}

{{ template "chart.description" . }}

{{ template "chart.versionBadge" . }}{{ template "chart.typeBadge" . }}{{ template "chart.appVersionBadge" . }}

## Overview

This Helm chart provides a comprehensive deployment solution for containerized applications on Kubernetes with support for:
- Progressive delivery strategies (Blue-Green, Canary) via Argo Rollouts
- Service mesh integration (Istio)
- Auto-scaling (HPA with custom metrics)
- Observability (Prometheus, Datadog)
- Security policies and RBAC
- AWS integration (IRSA, Pod Identity)

## Chart Components Architecture

### High-Level Deployment Architecture

```mermaid
graph TB
    subgraph "External Traffic"
        Users[Users/Clients]
        API[API Consumers]
    end

    subgraph "GitOps Platform"
        subgraph "CI/CD Cluster"
            ACD[ArgoCD Controller]
            AS[ApplicationSets]
            APP[Application CRDs]
            GIT[Git Repository]
        end
    end

    subgraph "Target AWS Account"
        subgraph "EKS Cluster"
            subgraph "Ingress Layer"
                CERT[Certificate Manager]
                IGW[Istio Gateway]
                VS[VirtualService]
                DR[DestinationRule]
            end

            subgraph "Application Namespace"
                subgraph "Workload Management"
                    RO[Argo Rollout/Deployment]
                    HPA[HorizontalPodAutoscaler]
                    PDB[PodDisruptionBudget]
                end

                subgraph "Pods"
                    POD1[Pod-1<br/>Main Container]
                    POD2[Pod-2<br/>Main Container]
                    PODN[Pod-N<br/>Main Container]
                end

                subgraph "Services"
                    SVC[Service<br/>ClusterIP]
                    PSVC[Preview Service]
                end

                subgraph "Configuration"
                    CM[ConfigMap<br/>Application Config]
                    CMENV[ConfigMap<br/>Env Vars]
                    SEC[Secrets<br/>Credentials]
                    SECENV[Secrets<br/>Env Vars]
                end
            end

            subgraph "Observability Stack"
                SM[ServiceMonitor]
                PR[PrometheusRule]
                PROM[Prometheus<br/>Metrics Collection]
                PA[Prometheus Adapter]
                DD[Datadog Integration]
                DASH[Grafana Dashboards]
            end

            subgraph "Security & RBAC"
                SA[ServiceAccount]
                ROLE[Role/ClusterRole]
                RB[RoleBinding]
                NP[NetworkPolicy]
                PSP[PodSecurityPolicy]
                PIA[PodIdentityAssociation]
            end
        end
    end

    subgraph "External Dependencies"
        AWS[AWS Services<br/>S3, RDS, SSM, etc.]
        KAFKA[Kafka/MSK<br/>Event Streaming]
        REDIS[Redis/ElastiCache<br/>Session Store]
        DB[Database<br/>RDS/Aurora]
        MON[Monitoring Backends<br/>Datadog, CloudWatch]
    end

    %% GitOps Flow
    GIT -->|Sync| AS
    AS -->|Generate| APP
    APP --> ACD
    ACD -->|Deploy| RO
    ACD -->|Deploy| CM
    ACD -->|Deploy| SEC

    %% Traffic Flow
    Users -->|HTTPS| IGW
    API -->|REST/gRPC| IGW
    IGW --> VS
    VS --> DR
    DR --> SVC
    SVC --> POD1
    SVC --> POD2
    SVC --> PODN

    %% Rollout Strategy
    RO -->|Manages| POD1
    RO -->|Manages| POD2
    RO -->|Manages| PODN
    PSVC -.->|Blue-Green| POD1

    %% Auto-scaling
    PA -->|Custom Metrics| HPA
    HPA -->|Scale| RO
    PROM -->|Metrics| PA

    %% Configuration
    CM -->|Mount| POD1
    CMENV -->|Env Vars| POD1
    SEC -->|Mount| POD1
    SECENV -->|Env Vars| POD1

    %% Monitoring
    SM -->|Discovery| PROM
    POD1 -->|Metrics| PROM
    PR -->|Alerts| PROM
    POD1 -->|APM| DD
    DD -->|Metrics| MON

    %% AWS Integration
    SA -.->|IRSA/Pod Identity| AWS
    POD1 -->|API Calls| AWS
    POD1 -->|Events| KAFKA
    POD1 -->|Cache| REDIS
    POD1 -->|Queries| DB

    %% Security
    ROLE -->|Permissions| RB
    RB -->|Grants| SA
    SA -->|Identity| POD1
    NP -->|Controls| POD1
    PIA -.->|AWS Identity| POD1

```

## Template Structure

### Core Templates Hierarchy

```mermaid
graph TD
    subgraph "Helper Templates"
        H[_helpers.tpl]
        HDEP[_deployment.tpl]
        HIST[_istio.tpl]
        HDD[_datadog.tpl]
    end

    subgraph "Workload Templates"
        DEP[deployment.yaml]
        RO[rollout.yaml]
        RAT[rollout-analysis-template.yaml]
    end

    subgraph "Service Templates"
        SVC[service.yaml]
        PSVC[rollout-preview-service.yaml]
        SM[servicemonitor.yaml]
    end

    subgraph "Configuration Templates"
        CM[configmap.yaml]
        CMENV[configmapsEnVars.yaml]
        SEC[secrets.yaml]
        SECENV[secretsEnvVars.yaml]
        DASH[configmap-dashboards.yaml]
    end

    subgraph "Scaling Templates"
        HPA[hpa.yaml]
        PDB[poddisruptionbudget.yaml]
        PAC[prometheus-adapter-config.yaml]
    end

    subgraph "Security Templates"
        SA[serviceaccount.yaml]
        ROLE[role.yaml]
        CROLE[clusterrole.yaml]
        RB[rolebinding.yaml]
        CRB[clusterrolebinding.yaml]
        NP[networkpolicy.yaml]
        PSP[podsecuritypolicy.yaml]
        PIA[podidentityassociation.yaml]
    end

    subgraph "Istio Templates"
        IGW[istio-gateway.yaml]
        IVS[istio-virtualservice.yaml]
        IDR[istio-destinationrule.yaml]
        IWP[istio-waypoint.yaml]
    end

    subgraph "Monitoring Templates"
        PR[prometheusrule.yaml]
        BACK[backup.yaml]
    end

    subgraph "TLS Templates"
        CERT[certificate.yaml]
    end

    H -->|Common Functions| DEP
    H -->|Common Functions| RO
    HDEP -->|Deployment Specs| DEP
    HDEP -->|Deployment Specs| RO
    HIST -->|Istio Config| IGW
    HIST -->|Istio Config| IVS
    HDD -->|Datadog Config| DEP
    HDD -->|Datadog Config| RO
```

## Deployment Strategies

### Progressive Delivery with Argo Rollouts

```mermaid
stateDiagram-v2
    [*] --> Deployment

    Deployment --> BlueGreen: rollout.strategy.blueGreen.enabled
    Deployment --> Canary: rollout.strategy.canary.enabled
    Deployment --> Standard: rollout.enabled = false

    state BlueGreen {
        Blue --> Preview
        Preview --> Promote: Manual/Auto
        Preview --> Abort: Failed
        Promote --> Green
        Green --> [*]
    }

    state Canary {
        Stable --> Canary_5: Step 1
        Canary_5 --> Canary_25: Step 2
        Canary_25 --> Canary_50: Step 3
        Canary_50 --> Canary_100: Step 4
        Canary_100 --> [*]

        Canary_5 --> Rollback: Analysis Failed
        Canary_25 --> Rollback: Analysis Failed
        Canary_50 --> Rollback: Analysis Failed
    }

    state Standard {
        RollingUpdate --> Complete
        Complete --> [*]
    }
```

### Service Mesh Traffic Flow (Istio)

```mermaid
flowchart LR
    subgraph "External Traffic"
        INET[Internet]
        ALB[AWS ALB/NLB]
    end

    subgraph "Istio Ingress"
        IGW[Istio Gateway]
        VS[VirtualService]
        DR[DestinationRule]
    end

    subgraph "Service Versions"
        STABLE[Stable Pods<br/>v1]
        CANARY[Canary Pods<br/>v2]
        PREVIEW[Preview Pods<br/>Blue-Green]
    end

    subgraph "Traffic Distribution"
        SPLIT{Traffic<br/>Split}
    end

    INET --> ALB
    ALB --> IGW
    IGW --> VS
    VS --> DR
    DR --> SPLIT

    SPLIT -->|Weight: 95%| STABLE
    SPLIT -->|Weight: 5%| CANARY
    SPLIT -.->|Preview Only| PREVIEW

    style CANARY fill:#ffcc00
    style PREVIEW fill:#00ccff
    style STABLE fill:#00ff00
```

## Observability Stack

### Metrics Collection and Alerting

```mermaid
graph TB
    subgraph "Application Pods"
        POD1[Pod 1<br/>/metrics]
        POD2[Pod 2<br/>/metrics]
        PODN[Pod N<br/>/metrics]
    end

    subgraph "Prometheus Stack"
        SM[ServiceMonitor]
        PROM[Prometheus<br/>Server]
        PR[PrometheusRule]
        AM[AlertManager]
    end

    subgraph "Custom Metrics"
        PA[Prometheus<br/>Adapter]
        MAPI[Metrics API<br/>Server]
        HPA[HorizontalPod<br/>Autoscaler]
    end

    subgraph "Datadog Integration"
        DD[Datadog Agent]
        DDAPM[APM Traces]
        DDLOGS[Log Collection]
    end

    subgraph "Dashboards"
        GRAF[Grafana]
        DASH[Dashboard<br/>ConfigMaps]
    end

    POD1 -->|Scrape| SM
    POD2 -->|Scrape| SM
    PODN -->|Scrape| SM

    SM --> PROM
    PR --> PROM
    PROM --> AM
    PROM --> PA
    PA --> MAPI
    MAPI --> HPA

    POD1 -->|Traces| DD
    POD2 -->|Logs| DD
    DD --> DDAPM
    DD --> DDLOGS

    PROM --> GRAF
    DASH --> GRAF

    HPA -->|Scale| POD1
    HPA -->|Scale| POD2
    HPA -->|Scale| PODN
```

## Security Architecture

### RBAC and Network Policies

```mermaid
graph LR
    subgraph "Identity & Access"
        SA[ServiceAccount]
        IRSA[IAM Role<br/>for SA]
        PIA[Pod Identity<br/>Association]
    end

    subgraph "RBAC"
        ROLE[Role/ClusterRole]
        RB[RoleBinding]
        CRB[ClusterRoleBinding]
    end

    subgraph "Network Security"
        NP[NetworkPolicy]
        PSP[PodSecurityPolicy]
        PSS[Pod Security<br/>Standards]
    end

    subgraph "Pod"
        CONT[Container]
        VOL[Volumes]
        ENV[Environment]
    end

    subgraph "AWS Resources"
        S3[S3 Buckets]
        RDS[RDS Database]
        SSM[SSM Parameters]
    end

    SA --> ROLE
    ROLE --> RB
    RB --> CONT

    SA --> IRSA
    IRSA --> S3
    IRSA --> RDS
    IRSA --> SSM

    SA --> PIA
    PIA --> S3

    NP -->|Ingress Rules| CONT
    NP -->|Egress Rules| CONT
    PSP -->|Security Context| CONT
    PSS -->|Admission Control| CONT
```

## Configuration Management

### ConfigMaps and Secrets Flow

```mermaid
flowchart TD
    subgraph "Configuration Sources"
        GIT[Git Repository]
        VAULT[HashiCorp Vault]
        SSM[AWS SSM<br/>Parameter Store]
        SOPS[SOPS Encrypted<br/>Secrets]
    end

    subgraph "Kubernetes Resources"
        CM[ConfigMap<br/>Application Config]
        CMENV[ConfigMap<br/>Environment Variables]
        SEC[Secret<br/>Credentials]
        SECENV[Secret<br/>Environment Variables]
    end

    subgraph "Pod Configuration"
        MOUNT[Volume Mounts<br/>/config/app.yaml]
        ENV[Environment Variables<br/>DATABASE_URL, API_KEY]
        INIT[Init Containers<br/>Config Validation]
    end

    GIT --> CM
    GIT --> CMENV
    VAULT --> SEC
    SSM --> SEC
    SOPS --> SEC

    CM -->|Mount| MOUNT
    CMENV -->|Env From| ENV
    SEC -->|Mount| MOUNT
    SECENV -->|Env From| ENV

    MOUNT --> INIT
    ENV --> INIT
    INIT -->|Validated Config| CONT[Main Container]
```

```mermaid
graph TB
    subgraph "Application Namespace"
        subgraph "Application Pods"
            APP[Metrics Endpoint<br/>:5555/metrics]
            JMX[JMX Port<br/>:9016]
        end

        SM[ServiceMonitor<br/>app: base]
        SVC[Metrics Service<br/>Port: 5555]
        HPA[HorizontalPodAutoscaler]
    end

    subgraph "Monitoring Namespace"
        subgraph "kube-prometheus-stack"
            PROM[Prometheus Server]
            AM[AlertManager]
            GO[Grafana]
            PO[Prometheus Operator]
        end

        subgraph "prometheus-adapter"
            PA[Custom Metrics API<br/>v1beta1.custom.metrics.k8s.io]
            PAC[Adapter Config]
        end

        subgraph "Datadog Agent"
            DSD[DogStatsD<br/>Unix Socket]
            APM[APM Agent]
            LOGS[Log Collector]
        end
    end

    subgraph "Metrics Flow"
        APP --> SVC
        SM -.->|Selects| SVC
        PO -.->|Creates| PROM
        PO -.->|Watches| SM
        PROM -->|Scrapes| SVC
        PROM -->|Stores| TS[(Time Series DB)]

        PA -->|Queries| PROM
        PAC -.->|Rules| PA
        PA -->|Exposes| API[Metrics API]
        HPA -->|Queries| API

        APP --> DSD
        JMX --> DSD
        APP --> APM
        APP --> LOGS

        GO -->|Queries| PROM
        AM -->|Alerts| PROM
    end

    subgraph "Metric Types"
        M1[realtimeeventfeed_connections_current]
        M2[realtimeeventfeed_event_latency]
        M3[realtimeeventfeed_kafka_stream_*]
        M4[jvm_memory_*]
        M5[process_cpu_*]
    end

    style PO fill:#9ff,stroke:#333,stroke-width:2px
    style PA fill:#9ff,stroke:#333,stroke-width:2px
```

### 4. Configuration Management

The chart provides a sophisticated configuration management system that handles both file-based and environment variable configurations, with intelligent merging between ConfigMaps and Secrets.

```mermaid
graph TD
    subgraph "Configuration Sources"
        VALUES["values.yaml"]
        ENVOVR["Environment<br/>Overrides"]
        GITOPS["GitOps/ArgoCD<br/>Overrides"]
    end

    subgraph "Helm Processing"
        PROC["Template Processing"]
        VALIDATION["Values Validation"]
        CHECKSUM["Checksum Generation"]
    end

    subgraph "Kubernetes Resources"
        CM["ConfigMap<br/>configMap"]
        CMENV["ConfigMap<br/>configMapEnvVars"]
        SEC["Secret<br/>secrets"]
        SECENV["Secret<br/>secretsEnvVars"]
        ENVVARS["Pod env<br/>envVars"]
    end

    subgraph "Init Container Processing"
        INIT["config-merger<br/>Init Container"]
        MERGE["Intelligent Merge<br/>Properties Files"]
        COPY["Copy Non-Properties<br/>Files"]
    end

    subgraph "Runtime Configuration"
        CONFDIR["/app/conf/<br/>Merged Config"]
        ENVRUNTIME["Environment<br/>Variables"]
        VOLS["Volume Mounts"]
    end

    subgraph "Application Access"
        APP["Application"]
        PROPS["Properties Loading"]
        ENVACCESS["Environment Access"]
    end

    VALUES --> PROC
    ENVOVR --> PROC
    GITOPS --> PROC
    
    PROC --> VALIDATION
    VALIDATION --> CHECKSUM
    CHECKSUM --> CM
    CHECKSUM --> CMENV
    CHECKSUM --> SEC
    CHECKSUM --> SECENV
    CHECKSUM --> ENVVARS

    CM --> INIT
    SEC --> INIT
    INIT --> MERGE
    INIT --> COPY
    MERGE --> CONFDIR
    COPY --> CONFDIR

    CMENV --> ENVRUNTIME
    SECENV --> ENVRUNTIME
    ENVVARS --> ENVRUNTIME

    CONFDIR --> VOLS
    VOLS --> APP
    ENVRUNTIME --> APP
    APP --> PROPS
    APP --> ENVACCESS

    style INIT fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    style MERGE fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    style CONFDIR fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
```

#### Configuration Processing Flow

1. **Template Processing**: Helm processes values.yaml and generates Kubernetes resources
2. **Resource Creation**: Four types of configuration resources are created:
   - **ConfigMap (configMap)**: File-based configuration (.properties files)
   - **ConfigMap (configMapEnvVars)**: Environment variables from ConfigMaps
   - **Secret (secrets)**: Sensitive file-based configuration
   - **Secret (secretsEnvVars)**: Sensitive environment variables

3. **Init Container Merge**: The `config-merger` init container intelligently merges configurations:
   - **Properties Files**: Merges .properties files from both ConfigMap and Secret
   - **Non-Properties Files**: Copies all other files from both sources
   - **Precedence**: Secret values override ConfigMap values for same keys

4. **Runtime Delivery**: Applications access configuration through:
   - **Volume Mounts**: Merged files in `/app/conf/`
   - **Environment Variables**: Loaded from configMapEnvVars and secretsEnvVars
   - **Direct Access**: Original ConfigMap and Secret mounts available

#### Configuration Types and Usage

```yaml
# values.yaml configuration structure
configMap:
  application.properties:
    server.port: 8080
    kafka.bootstrap.servers: "localhost:9092"
    logging.level.com.viafoura: DEBUG
  
  vfmetrics.properties:
    metrics.port: 5555
    metrics.path: /metrics
    metrics.enabled: true

secrets:
  application.properties:
    kafka.username: "secure-user"
    kafka.password: "secure-password"
    database.password: "db-secret"
  
  credentials.properties:
    api.key: "secret-api-key"
    jwt.secret: "signing-secret"

configMapEnvVars:
  LOG_LEVEL: "INFO"
  DEBUG_MODE: "false"
  API_TIMEOUT: "30s"

secretsEnvVars:
  DATABASE_PASSWORD: "postgres-password"
  API_TOKEN: "external-service-token"

envVars:
  JAVA_OPTIONS: "-Xms100m -Xmx100m"  # Example for Java services
```

#### Merge Behavior

The init container performs intelligent merging:

1. **Properties Files with Same Name**:
   ```properties
   # Final application.properties (merged)
   # ConfigMap properties
   server.port=8080
   kafka.bootstrap.servers=localhost:9092
   
   # Secret properties (override ConfigMap)
   kafka.username=secure-user
   kafka.password=secure-password
   ```

2. **File Precedence**:
   - Properties files: ConfigMap + Secret (merged)
   - Non-properties files: Secret overrides ConfigMap if same filename
   - Unique files: Copied from respective sources

3. **Volume Mount Structure**:
   ```bash
   /app/conf/                    # Merged configuration (primary)
   ├── application.properties    # Merged ConfigMap + Secret
   ├── vfmetrics.properties     # From ConfigMap only
   └── credentials.properties   # From Secret only
   
   /app/configmap/              # Original ConfigMap files
   /app/secrets/                # Original Secret files
   ```

#### Security Considerations

- **Checksum Updates**: Configuration changes trigger pod restarts via checksum annotations
- **Read-Only Access**: Configuration volumes are mounted read-only in main container
- **Secure Init**: Init container runs with minimal security context
- **Separation**: Clear separation between sensitive and non-sensitive configuration

## Monitoring Stack Integration

### kube-prometheus-stack Integration

The chart integrates with the **kube-prometheus-stack** (v0.83.0) for comprehensive monitoring:

1. **ServiceMonitor Resource**:
   - Automatically discovered by Prometheus Operator
   - Labeled with `release: kube-prometheus-stack` for proper selection
   - Configures scraping of metrics endpoint every 10s

2. **Prometheus Server**:
   - Deployed in `monitoring` namespace
   - Scrapes metrics from all pods via the metrics service
   - Stores time-series data for querying

3. **Grafana Integration**:
   - Custom dashboard included in ConfigMap
   - Visualizes application-specific metrics

### prometheus-adapter Integration

The **prometheus-adapter** (v0.12.0) enables custom metrics for autoscaling:

1. **Custom Metrics API**:
   - Exposes Prometheus metrics as Kubernetes custom metrics
   - Implements `custom.metrics.k8s.io/v1beta1` API

2. **HPA Integration**:
   - HPA queries custom metrics like `realtimeeventfeed_connections_current`
   - Enables scaling based on business metrics, not just CPU/memory

3. **Configuration**:
   ```yaml
   # Example prometheus-adapter rule
   rules:
   - seriesQuery: 'realtimeeventfeed_connections_current'
     resources:
       overrides:
         namespace: {resource: "namespace"}
         pod: {resource: "pod"}
     name:
       matches: "^realtimeeventfeed_(.*)$"
       as: "${1}"
     metricsQuery: 'avg_over_time(<<.Series>>{<<.LabelMatchers>>}[2m])'
   ```

## Key Features

### 1. High Availability
- **Multi-replica deployment** with pod anti-affinity
- **PodDisruptionBudget** ensuring minimum availability
- **Health checks**: Liveness, Readiness, and Startup probes
- **Automatic scaling** based on CPU, memory, or custom metrics

### 2. Security
- **ServiceAccount with IRSA** for AWS integration
- **Network policies** via Istio
- **TLS termination** at Gateway level
- **Secrets management** with encryption

### 3. Observability
- **Prometheus metrics** with ServiceMonitor
- **Datadog integration** for APM and logs
- **Distributed tracing** support
- **Custom dashboards** included

### 4. Traffic Management
- **Progressive delivery** with canary deployments
- **A/B testing** capabilities
- **Circuit breaking** and retry policies
- **Load balancing** with connection pooling

## Deployment Flow

```mermaid
sequenceDiagram
    participant Dev as Developer
    participant Git as Git Repository
    participant ArgoCD as ArgoCD (CICD Cluster)
    participant K8s as Kubernetes (App Cluster)
    participant PO as Prometheus Operator
    participant Argo as Argo Rollouts
    participant Istio as Istio
    participant App as Application
    participant Prom as kube-prometheus-stack

    Dev->>Git: Push changes
    ArgoCD->>Git: Detect changes
    ArgoCD->>ArgoCD: ApplicationSet generates manifests
    ArgoCD->>K8s: Cross-account sync
    K8s->>K8s: Create ConfigMaps/Secrets
    K8s->>K8s: Create ServiceAccount
    K8s->>K8s: Create ServiceMonitor
    PO->>PO: Detect ServiceMonitor
    PO->>Prom: Configure Scrape Target
    K8s->>Argo: Create/Update Rollout
    Argo->>K8s: Create Pods (Canary)
    K8s->>App: Start Application
    App->>App: Load Configuration
    App->>App: Connect to Kafka
    App->>App: Start Metrics Server (:5555)
    K8s->>Istio: Configure Gateway/VS/DR
    Istio->>Istio: Setup Traffic Rules (5% canary)
    App->>App: Start Health Endpoints
    K8s->>K8s: Probes Succeed
    Argo->>Argo: Mark as Ready
    Prom->>App: Scrape /metrics
    Argo->>Prom: Query Success Rate (Analysis)
    Argo->>Argo: Progressive traffic shift
    Istio->>App: Route Traffic (100%)
```

## Resource Hierarchy

```yaml
Namespace
├── ServiceAccount (with IAM role)
├── ConfigMaps
│   ├── application.properties
│   ├── vfmetrics.properties
│   └── dashboard.json
├── Secrets
│   ├── API credentials
│   └── Environment secrets
├── Rollout (Argo)
│   ├── ReplicaSet (stable)
│   │   └── Pods
│   └── ReplicaSet (canary)
│       └── Pods
├── Services
│   ├── Main Service (8080)
│   └── Metrics Service (5555)
├── Monitoring
│   ├── ServiceMonitor
│   └── HorizontalPodAutoscaler
├── Istio Resources
│   ├── Gateway
│   ├── VirtualService
│   ├── DestinationRule
│   └── EnvoyFilter
└── PodDisruptionBudget
```

## Kubernetes Resource Calculation Guidelines

### Overview
This document provides formulas for calculating optimal Kubernetes resource values for containerized applications, with examples for JVM-based services using `-Xms100m -Xmx100m` heap settings.

### Memory Calculation Formula

#### Base Components
```
Total Container Memory = Heap + Non-Heap + Direct Memory + JVM Overhead + OS Buffer

Where:
- Heap = -Xmx value
- Non-Heap = Metaspace + Code Cache + Thread Stacks + JVM Internal
- Direct Memory = MaxDirectMemorySize (defaults to -Xmx if not set)
- JVM Overhead = ~8-15% of (Heap + Non-Heap)
- OS Buffer = ~10-20MB for container processes
```

#### Detailed Memory Request Calculation

For `-Xmx100m`:

```
Memory Request Calculation:
━━━━━━━━━━━━━━━━━━━━━━━━━
Heap Memory          = 100 MB  (-Xmx100m)
Metaspace           ≈  30 MB  (default ~20-50MB)
Code Cache          ≈  15 MB  (default ~15-50MB)
Thread Stacks       ≈  10 MB  (10 threads × 1MB stack)
Other JVM Native    ≈  10 MB  (GC structures, symbols)
Direct Memory       ≈  50 MB  (Netty buffers, ~50% of heap)
JVM Overhead        ≈  20 MB  (~10% of above)
OS/Container Buffer ≈  20 MB
                      -------
Total               ≈ 255 MB → Round to 256Mi (power of 2)
```

#### Memory Limit Calculation

```
Memory Limit Calculation:
━━━━━━━━━━━━━━━━━━━━━━━━
Base (from above)   = 255 MB
Safety Factor       = 2x      (for spikes, GC, peak load)
                      -------
Total               = 510 MB → Round to 512Mi
```

### CPU Calculation Formula

#### Base Formula
```
CPU Request = max(JVM threads × 10m, 100m)
CPU Limit = CPU Request × 3-5x (for burst capacity)

Where:
- JVM threads ≈ 10-20 for typical Java app
- 10m = 0.01 CPU core per thread baseline
- Minimum 100m for scheduling priority
```

#### Application-Specific CPU Calculation

```
CPU Request Calculation:
━━━━━━━━━━━━━━━━━━━━━━━
Event Loop Threads  = 2 × cores (framework default)
Worker Threads      = 20 (typical default)
GC Threads          = 2-4
Other JVM Threads   = 5-10
                      -------
Total Threads       ≈ 30-40
Min Baseline        = 30 × 3m = 90m
Recommended Min     = 100m (for better scheduling)

CPU Limit Calculation:
━━━━━━━━━━━━━━━━━━━━━
Base Request        = 100m
Burst Factor        = 5x (startup, peak loads)
                      -------
Total               = 500m
```

### Generic Formulas

#### Memory Formula
```python
# Memory Request (Mi)
memory_request = heap + (heap × direct_memory_factor) + metaspace + overhead + buffer

# Memory Limit (Mi)
memory_limit = memory_request × safety_factor

# Where:
# direct_memory_factor = 0.5-1.0 (depends on Netty usage)
# metaspace = 30-50 MB
# overhead = (heap + non_heap) × 0.1
# buffer = 20 MB
# safety_factor = 1.5-2.0
```

#### CPU Formula
```python
# CPU Request (millicores)
cpu_request = max(minimum_cpu, active_threads × cpu_per_thread)

# CPU Limit (millicores)
cpu_limit = cpu_request × burst_factor

# Where:
# minimum_cpu = 100m
# active_threads = event_loops + workers + jvm_threads
# cpu_per_thread = 3-10m
# burst_factor = 3-5
```

### Quick Reference Table

| JVM Heap (-Xmx) | Memory Request | Memory Limit | CPU Request | CPU Limit |
|-----------------|----------------|--------------|-------------|-----------|
| 64m             | 192Mi          | 384Mi        | 100m        | 500m      |
| 100m            | 256Mi          | 512Mi        | 100m        | 500m      |
| 256m            | 512Mi          | 1Gi          | 200m        | 1000m     |
| 512m            | 1Gi            | 2Gi          | 250m        | 1000m     |
| 1g              | 2Gi            | 4Gi          | 500m        | 2000m     |
| 2g              | 4Gi            | 6Gi          | 500m        | 2000m     |

### Key Multipliers

| Component | Multiplier | Reason |
|-----------|-----------|---------|
| Direct Memory | 0.5-1.0 × Heap | Netty allocates off-heap buffers |
| Non-Heap | 0.5-0.8 × Heap | Metaspace, code cache, threads |
| Safety Factor | 2.0x | GC spikes, peak loads |
| CPU Burst | 3-5x | Startup, compilation, peak processing |

### Tuning Guidelines

#### When to Increase Memory
- More Netty channels/connections: Increase direct memory factor to 0.8-1.0
- More threads: Add 1MB per additional thread
- Complex application: Increase metaspace by 20-50%
- Frequent Full GCs: Increase heap and overall memory

#### When to Increase CPU
- High CPU throttling in metrics
- Slow application startup
- Complex computational workloads
- Many concurrent requests

#### When to Decrease Resources
- Stable load patterns: Reduce safety factor to 1.5x
- Simple applications: Reduce metaspace allocation
- Few connections: Reduce direct memory factor to 0.3-0.5

### Example Configurations

#### Minimal Service
```yaml
resources:
  requests:
    memory: "256Mi"
    cpu: "100m"
  limits:
    memory: "512Mi"
    cpu: "500m"
```

#### Standard Service
```yaml
resources:
  requests:
    memory: "512Mi"
    cpu: "250m"
  limits:
    memory: "1Gi"
    cpu: "1000m"
```

#### High-Load Service
```yaml
resources:
  requests:
    memory: "2Gi"
    cpu: "1000m"
  limits:
    memory: "4Gi"
    cpu: "4000m"
```

### Monitoring Commands

```bash
# Check actual memory usage
kubectl top pods -n your-namespace

# Get detailed JVM memory breakdown
kubectl exec -it your-pod -- jcmd 1 VM.native_memory summary

# Check for CPU throttling
kubectl exec -it your-pod -- cat /sys/fs/cgroup/cpu/cpu.stat | grep throttled
```

### Notes

1. These formulas provide starting points; always validate with actual metrics
2. Container memory limits should be ~20% higher than JVM memory to prevent OOMKilled
3. Use HorizontalPodAutoscaler for production workloads
4. Consider using `-XX:+UseContainerSupport` for better container awareness
5. Monitor and adjust based on actual usage patterns

## Configuration Options

### Essential Values

```yaml
# Core Configuration
replicaCount: 2
image:
  repository: aws-ecr-url
  tag: v1.0.0

# Service Configuration
service:
  port: 8080
  containerPorts:
    metrics:
      enabled: true
      exposeService: true

# Autoscaling
autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 5
  targetCPUUtilizationPercentage: 33

# Monitoring
serviceMonitor:
  enabled: true
  prometheusReleaseName: kube-prometheus-stack

# Deployment Strategy
rollout:
  enabled: true
  strategy:
    canary:
      enabled: true
      steps: [5%, 10%, 20%, 50%, 100%]
  analysis:
    prometheus:
      # References in-cluster Prometheus from kube-prometheus-stack
      address: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090
```

## Best Practices

1. **Resource Management**
   - Set appropriate resource requests/limits
   - Use HPA for dynamic scaling
   - Configure PDB for availability

2. **Security**
   - Enable IRSA for AWS access
   - Use secrets for sensitive data
   - Configure network policies

3. **Monitoring**
   - Enable ServiceMonitor for Prometheus
   - Configure Datadog for APM
   - Set up alerts for critical metrics

4. **Deployment**
   - Use canary deployments for safety
   - Configure analysis for automatic rollback
   - Test in lower environments first

## Troubleshooting

See [TROUBLESHOOTING.md](./TROUBLESHOOTING.md) for common issues and solutions.

## Testing

The chart includes comprehensive tests:
- Unit tests for template rendering
- Integration tests for Kubernetes resources
- Monitoring tests for observability
- Security tests for compliance

Run tests with:

```bash
helm test <release-name>
```

## Future Enhancements

1. **Multi-region support** with federation across AWS accounts
2. **Advanced traffic management** with locality preferences
3. **Cost optimization** with spot instance support
4. **Enhanced security** with Pod Security Standards
5. **Multi-cluster ArgoCD** with sharding for scale
6. **Progressive delivery** across regions with global rollouts

## Installing the Chart

To install the chart with the release name `my-release`:

```console
$ helm repo add foo-bar http://charts.foo-bar.com
$ helm install my-release foo-bar/{{ template "chart.name" . }}
```

## Update Helm Schema and This README.md

```bash
helm-schema --add-schema-reference --helm-docs-compatibility-mode --append-newline && helm-docs .
```

---

{{ template "chart.requirementsSection" . }}

{{ template "chart.valuesSection" . }}

{{ template "helm-docs.versionFooter" . }}

