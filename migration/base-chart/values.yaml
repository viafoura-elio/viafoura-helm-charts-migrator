# Default values for base-chart.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# @schema
# enum: ["218894879100", "825534976873"]
# required: true
# additionalProperties: false
# @schema
# -- AWS Account ID
awsAccountId: "218894879100"

# @schema
# type: string
# additionalProperties: false
# @schema
# -- Override for chart name generation. When set, replaces the default chart name from Chart.yaml in resource naming
nameOverride: ""

# @schema
# type: string
# additionalProperties: false
# @schema
# -- Complete override for resource naming. When set, this exact name is used for all Kubernetes resources instead of generated names
fullnameOverride: ""

# @schema
# type: integer
# exclusiveMinimum: 0
# maximum: 500
# @schema
# -- Number of application pod replicas to maintain for high availability and load distribution
replicaCount: 1

# @schema
# additionalProperties: false
# @schema
# -- Kubernetes deployment strategy for managing pod updates and ensuring zero-downtime deployments
strategy:
  # -- Strategy type: RollingUpdate for gradual replacement, Recreate for immediate replacement
  type: RollingUpdate

  # @schema
  # additionalProperties: false
  # @schema
  # -- Kubernetes deployment strategy for managing pod updates and ensuring zero-downtime deployments
  rollingUpdate:
    # -- Maximum extra pods allowed during update (can be number or percentage like "25%")
    maxSurge: 1
    # -- Maximum pods that can be unavailable during update to maintain service availability
    maxUnavailable: 0

# @schema
# additionalProperties: false
# @schema
# -- Container image configuration for the base-chart application
image:

  # -- ECR Registry Name
  name: base-chart-service

  # -- Docker image repository URL
  repository: 218894879100.dkr.ecr.us-east-1.amazonaws.com

  # @schema
  # additionalProperties: false
  # enum: [IfNotPresent,Always,Never]
  # default: Always
  # @schema
  # -- Image pull policy: IfNotPresent (cache locally), Always (always pull), Never (use local only)
  pullPolicy: Always

  # -- Specific image tag to deploy. When empty, uses the chart's appVersion from Chart.yaml
  tag: "v0.0.1"

# @schema
# additionalProperties: true
# @schema
# -- List of secret names containing registry credentials for pulling images from private repositories like AWS ECR
# Example: [{name: "aws-ecr-secret"}]
imagePullSecrets: []

# @schema
# additionalProperties: false
# @schema
# -- ServiceAccount configuration for pod identity and AWS IAM integration
serviceAccount:
  # -- Create a dedicated ServiceAccount for this application (recommended for security)
  create: true
  # -- Automatically mount the ServiceAccount token for API access (required for most applications)
  automount: true
  # -- Custom annotations for the ServiceAccount (e.g., for AWS IAM role association)
  annotations: {}
  # -- Custom name for the ServiceAccount. If empty, generates from the release name
  name: ""
  # -- AWS IAM role integration for pod-level permissions via IRSA (IAM Roles for Service Accounts)
  # Note: Choose either IRSA or Pod Identity, not both
  iamRole:
    # -- Enable IAM role association for accessing AWS services like S3, RDS, etc.
    enabled: false
    # -- ARN of the IAM role to associate with this ServiceAccount
    # Example: "arn:aws:iam::218894879100:role/base-chart-pod-role"
    roleArn: ""
    # -- Specific IAM role name to associate. If empty, uses generated format: {fullnameOverride}.{namespace}.pod
    name: ""
  # -- AWS Pod Identity integration (newer alternative to IRSA)
  # Provides simplified IAM role association for pods without requiring OIDC configuration
  podIdentity:
    # -- Enable AWS Pod Identity for IAM role association
    enabled: true
    # -- ARN of the IAM role to associate with this ServiceAccount
    # Example: "arn:aws:iam::218894879100:role/livechat.vf-dev2.pod"
    roleArn: ""
    # -- Specific IAM role name to associate. If empty, uses generated format: {fullnameOverride}.{namespace}.pod
    name: ""

# @schema
# type: object
# additionalProperties: false
# @schema
# -- RBAC configuration for pod permissions with the least privilege principle# Example cluster rules:
# Example custom rules:
# rbac:
#   role:
#     rules:
#       - apiGroups: [""]
#         resources: ["configmaps"]
#         verbs: ["get", "list"]
#   clusterRole:
#     rules:
#       - apiGroups: [""]
#         resources: ["nodes"]
#         verbs: ["get", "list"]
rbac:
  # -- Enable RBAC resource creation
  create: true
  # -- Role configuration for namespace-scoped permissions
  role:
    # -- Create a Role for namespace-scoped permissions
    create: true
    # -- Custom annotations for the Role
    annotations: {}
    # -- Custom rules for the Role (will be merged with default minimal rules)
    # Use this to add additional permissions your application needs
    rules: []
  # -- ClusterRole configuration for cluster-scoped permissions (use sparingly)
  clusterRole:
    # -- Create a ClusterRole for cluster-scoped permissions
    # Only enable if your application absolutely needs cluster-wide access
    create: false
    # -- Custom annotations for the ClusterRole
    annotations: {}
    # -- Custom rules for the ClusterRole
    # Only add rules that require cluster-wide access
    rules: []

# @schema
# additionalProperties: true
# @schema
# -- Global annotations applied to all Kubernetes resources created by this chart
# Example: {"app.kubernetes.io/owner": "platform-team"}
defaultAnnotations: {}

# @schema
# additionalProperties: true
# @schema
# -- Custom annotations for pods, useful for monitoring, networking, and security policies
# Example: {"prometheus.io/scrape": "true", "linkerd.io/inject": "enabled"}
podAnnotations: {}

# @schema
# additionalProperties: true
# @schema
# -- Custom labels for pods used for selection, monitoring, and organization
# Example: {"team": "backend", "component": "event-processor"}
podLabels: {}

# @schema
# additionalProperties: true
# @schema
# -- Pod-level security context for controlling filesystem permissions and system settings
# Implements security best practices and least privilege principles
podSecurityContext:
  # -- Run as non-root user for security
  runAsNonRoot: true
  # -- Specific user ID to run the container (1000 is typically a safe non-root user)
  runAsUser: 1000
  # -- Specific group ID for the container
  runAsGroup: 1000
  # -- Set filesystem group ownership for volumes
  fsGroup: 1000
  # -- Ensure filesystem group ownership changes are applied to volumes
  fsGroupChangePolicy: "OnRootMismatch"
  # -- Supplemental groups for the security context
  supplementalGroups: []
  # -- Set the seccomp profile to restrict system calls
  seccompProfile:
    type: RuntimeDefault
  # -- Kernel parameters (sysctls) for pod-level tuning
  # Common for network stack optimization and connection handling
  # Example sysctls for high-throughput applications:
  # sysctls:
  #   - name: net.ipv4.ip_local_port_range
  #     value: "2048 64511"
  #   - name: net.core.somaxconn
  #     value: "16384"
  sysctls: []

# @schema
# additionalProperties: false
# @schema
# -- Container-level security context for privilege control and attack surface reduction
# Implements defense-in-depth security principles
securityContext:
  # -- Drop all Linux capabilities for maximum security
  capabilities:
    drop:
      - ALL
  # -- Set the filesystem as read only
  readOnlyRootFilesystem: true
  # -- Prevent running as root user
  runAsNonRoot: true
  # -- Specific user ID to run the container
  runAsUser: 1000
  # -- Specific group ID for the container
  runAsGroup: 1000
  # -- Prevent privilege escalation
  allowPrivilegeEscalation: false
  # -- Set the seccomp profile to restrict system calls
  seccompProfile:
    type: RuntimeDefault

# @schema
# additionalProperties: false
# @schema
# -- CPU and memory resource management for predictable performance and cluster stability
# Resource allocation ensures:
# - Predictable performance through guaranteed resources (requests)
# - Protection against resource exhaustion through limits
# - Proper scheduling and bin-packing by Kubernetes scheduler
# Memory calculation for JVM applications:
# - Heap memory: Application objects and data
# - Non-heap: ~20-25% of heap (metaspace, code cache, etc.)
# - Direct memory: Off-heap buffers, typically matches heap size
# - Safety buffer: 20% overhead for OS and container
resources:
  # -- Hard Resources Limits
  limits:
    # -- Maximum CPU cores (1 core = 1000m). Throttles CPU usage to prevent noisy neighbor issues
    cpu: 1
    # -- Maximum memory including heap, non-heap, and direct memory. Prevents OOM kills
    memory: 400Mi
  # -- Soft Resources Limits
  requests:
    # -- Guaranteed CPU allocation for consistent performance. Used by scheduler for placement
    cpu: 50m
    # -- Guaranteed memory allocation. Should match limits for memory-intensive applications
    memory: 400Mi

# @schema
# additionalProperties: true
# @schema
# -- Kubernetes Service configuration for network access and load balancing
service:
  # -- Short alias name used for service discovery and internal routing
  alias: base-chart-alias
  # -- Full service name for DNS resolution within the cluster
  name: base-chart
  # -- Service Annotations
  annotations: {}
  # -- Named port for the main HTTP service endpoint
  portName: http
  # -- Network protocol: TCP for HTTP/WebSocket, UDP for streaming protocols
  protocol: TCP
  # -- Service type: ClusterIP (internal), LoadBalancer (external), NodePort (development)
  type: ClusterIP
  # -- External port exposed by the Service for client connections
  port: 80
  # -- Internal port the container listens on for incoming requests
  targetPort: 8080
  # -- NodePort for development/debugging (only when type is NodePort)
  nodePort:
  # -- Additional container ports configuration (JMX, metrics)
  containerPorts:
    # -- JMX (Java Management Extensions) port for JVM monitoring and management
    jmx:
      # -- Enable JMX port for Java application monitoring with tools like JConsole, VisualVM
      enabled: false
      # -- Expose JMX port through the main Service (false for security - use port-forwarding instead)
      exposeService: false
      # -- Named port identifier for JMX endpoint
      containerPortName: jmx
      # -- Port number for JMX connections (standard JMX port range)
      containerPort: 9016
      # -- Protocol for JMX communication (typically TCP)
      containerProtocol: TCP
    # -- Prometheus metrics port for application performance monitoring
    metrics:
      # -- Enable metrics endpoint for Prometheus scraping and monitoring dashboards
      enabled: true
      # -- Expose metrics port through Service for ServiceMonitor discovery
      exposeService: true
      # -- Named port identifier for metrics endpoint
      containerPortName: metrics
      # -- Port number for Prometheus metrics scraping (/metrics endpoint)
      containerPort: 5555
      # -- Protocol for metrics communication
      containerProtocol: TCP

# @schema
# additionalProperties: true
# @schema
# Health check configuration for automatic failure detection and recovery
# -- Liveness probe determines if the container is healthy and should be restarted if failing
# Used for detecting deadlocks, infinite loops, or unrecoverable application states
livenessProbe:
  enabled: false
  config:
    # -- HTTP health check endpoint that returns 200 OK when application is functioning
    httpGet:
      path: /healthy
      port: 8080
      scheme: HTTP
    # -- Wait time before starting health checks to allow application startup (JVM warmup, dependency connections)
    initialDelaySeconds: 40
    # -- Number of consecutive check failures before restarting the container (prevents flapping)
    failureThreshold: 5
    # -- Frequency of health checks during normal operation
    periodSeconds: 30
    # -- Number of consecutive successes to mark container as healthy again after failure
    successThreshold: 1
    # -- Maximum time to wait for health check response before marking as failed
    timeoutSeconds: 10

# @schema
# additionalProperties: true
# @schema
# -- Readiness probe determines if the container can accept traffic (added/removed from Service endpoints)
# Used for controlling traffic flow during startup, rolling updates, and temporary unavailability
readinessProbe:
  enabled: false
  config:
    # -- HTTP endpoint check to verify application is ready to serve requests
    httpGet:
      path: /healthy
      port: 8080
      scheme: HTTP
    # -- Delay before starting readiness checks (should be less than liveness to avoid restart loops)
    initialDelaySeconds: 60
    # -- Number of consecutive successes required to mark pod as ready for traffic
    successThreshold: 1
    # -- Number of consecutive failures before removing pod from Service load balancing
    failureThreshold: 5
    # -- Frequency of readiness checks (more frequent than liveness for responsive traffic management)
    periodSeconds: 10
    # -- Timeout for readiness check response
    timeoutSeconds: 5

# @schema
# additionalProperties: true
# @schema
# -- Startup probe protects slow-starting containers from being killed by liveness probe during initialization
# Gives applications extended time to complete startup procedures like database migrations, cache warming
startupProbe:
  enabled: false
  config:
    # -- HTTP endpoint to verify application has completed startup initialization
    httpGet:
      path: /healthy
      port: 8080
      scheme: HTTP
    # -- Delay before first startup check (minimal since this probe handles slow startups)
    initialDelaySeconds: 15
    # -- Maximum startup check failures allowed (total startup time = failureThreshold * periodSeconds)
    failureThreshold: 30
    # -- Frequency of startup checks during application initialization
    periodSeconds: 10
    # -- Single success marks startup complete, enabling liveness/readiness probes
    successThreshold: 1
    # -- Timeout for startup check response
    timeoutSeconds: 5

# @schema
# additionalProperties: true
# autoscaling:
# @schema
# -- Horizontal Pod Autoscaler for automatic scaling based on resource utilization and custom metrics
autoscaling:
  # -- Enable automatic pod scaling to handle varying traffic loads
  enabled: true
  # -- Minimum pod count to maintain for baseline capacity and availability
  minReplicas: 1
  # -- Maximum pod count to prevent resource exhaustion and cost overruns
  maxReplicas: 3
  # -- CPU utilization threshold
  targetCPUUtilizationPercentage: 70
  # -- Memory utilization threshold
  targetMemoryUtilizationPercentage: 80

  # @schema
  # type: array
  # items:
  #   type: object
  #   properties:
  #     name:
  #       type: string
  #     type:
  #       type: string
  #       enum: [Pods, Object, External]
  #     target:
  #       type: object
  #     selector:
  #       type: object
  #     describedObject:
  #       type: object
  # @schema
  # -- Custom metrics from Prometheus via Prometheus Adapter for advanced autoscaling
  # Supports Pods, Object, and External metric types from ServiceMonitor/Prometheus
  # Example for Pods type (most common):
  #   - name: http_requests_per_second
  #     type: Pods
  #     target:
  #       type: AverageValue
  #       averageValue: "1000"
  # Example for External type:
  #   - name: queue_depth
  #     type: External
  #     selector:
  #       matchLabels:
  #         queue: base-chart
  #     target:
  #       type: AverageValue
  #       averageValue: "30"
  customMetrics: []

  # @schema
  # additionalProperties: true
  # @schema
  # -- Horizontal Pod Autoscaler Behavior
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 10
          periodSeconds: 60
        - type: Pods
          value: 1
          periodSeconds: 60
      selectPolicy: Min
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Percent
          value: 100
          periodSeconds: 60
        - type: Pods
          value: 2
          periodSeconds: 60
      selectPolicy: Max

# @schema
# additionalProperties: true
# volumes:
# @schema
# -- Additional volumes for configuration files, secrets, or persistent storage
# Includes default temporary volumes for read-only filesystem compatibility
# Additional volume examples:
# - name: config-volume       # ConfigMap volume
#   configMap:
#     name: app-config
# - name: secret-volume       # Secret volume for certificates
#   secret:
#     secretName: tls-certs
#     optional: false
# - name: shared-storage      # Persistent volume
#   persistentVolumeClaim:
#     claimName: shared-data
volumes: []

# @schema
# additionalProperties: true
# volumeMounts:
# @schema
# -- Mount points for additional volumes inside the container filesystem
# Includes default mounts for read-only filesystem compatibility
# Default volume mounts for read-only root filesystem
# Additional mount examples:
# - name: config-volume
#   mountPath: "/app/config"       # Application configuration directory
#   readOnly: true
# - name: secret-volume
#   mountPath: "/etc/ssl/certs"    # SSL certificate location
#   readOnly: true
# - name: shared-storage
#   mountPath: "/shared"           # Shared data directory
#   readOnly: false
volumeMounts: []

# @schema
# arch:
#   type: string
#   enum: ["amd64", "arm64"]
# @schema
# -- Target CPU architecture for container scheduling and node selection
# Ensures pods run on compatible nodes (amd64 for Intel/AMD, arm64 for ARM processors)
arch: amd64

# @schema
# additionalProperties: true
# @schema
# -- Pod Disruption Budget to maintain availability during cluster maintenance and updates
podDisruptionBudget:
  # -- Enable Pod Disruption Budget
  enabled: false
  # -- Maximum pods that can be unavailable during voluntary disruptions (node drains, updates)
  # Alternative: minAvailable: 1  # Minimum pods that must remain available
  maxUnavailable: 1

# @schema
# additionalProperties: true
# @schema
# -- Node selector labels for scheduling pods on specific nodes with required characteristics
# Use for dedicated nodes, specific hardware, or compliance requirements
# Examples:
# dedicated: base-chart
# Dedicated node pool
# node-type: compute-optimized       # High-performance nodes
# zone: us-east-1a                   # Specific availability zone
nodeSelector: {}

# @schema
# additionalProperties: true
# type: array
# @schema
# -- Tolerations allow pods to schedule on nodes with matching taints
# Used for dedicated nodes, special hardware, or workload isolation
tolerations: []

# @schema
# additionalProperties: true
# @schema
# -- Advanced pod scheduling rules for controlling pod placement relative to other pods and nodes
# Provides fine-grained control over where pods are scheduled in the cluster for performance, compliance, and availability requirements.
# Use cases:
# - Co-location: Schedule pods near each other (e.g., app with cache)
# - Anti-affinity: Spread pods across nodes/zones for high availability
# - Node affinity: Target specific hardware, zones, or node types
# - Mixed workload isolation: Separate different application tiers
# Example configurations:
# # Pod Anti-Affinity: Spread pods across different nodes for high availability
# podAntiAffinity:
#   preferredDuringSchedulingIgnoredDuringExecution:
#     - weight: 100
#       podAffinityTerm:
#         labelSelector:
#           matchExpressions:
#             - key: app.kubernetes.io/name
#               operator: In
#               values:
#                 - base-chart
#         topologyKey: kubernetes.io/hostname
#
# # Zone Anti-Affinity: Distribute pods across availability zones
# podAntiAffinity:
#   preferredDuringSchedulingIgnoredDuringExecution:
#     - weight: 80
#       podAffinityTerm:
#         labelSelector:
#           matchLabels:
#             app.kubernetes.io/name: base-chart
#         topologyKey: topology.kubernetes.io/zone
#
# # Node Affinity: Prefer compute-optimized nodes for CPU-intensive workloads
# nodeAffinity:
#   preferredDuringSchedulingIgnoredDuringExecution:
#     - weight: 100
#       preference:
#         matchExpressions:
#           - key: node-type
#             operator: In
#             values:
#               - compute-optimized
#               - cpu-optimized
#
# # Required Node Affinity: Must run on nodes with SSD storage
# nodeAffinity:
#   requiredDuringSchedulingIgnoredDuringExecution:
#     nodeSelectorTerms:
#       - matchExpressions:
#           - key: storage-type
#             operator: In
#             values:
#               - ssd
#
# # Pod Affinity: Co-locate with Redis cache for low latency
# podAffinity:
#   preferredDuringSchedulingIgnoredDuringExecution:
#     - weight: 100
#       podAffinityTerm:
#         labelSelector:
#           matchLabels:
#             app: redis
#         topologyKey: kubernetes.io/hostname
affinity: {}

# @schema
# additionalProperties: true
# @schema
# -- Pod affinity configuration rules
# - enabled: true
#   weight: 100
#   topologyKey: "kubernetes.io/hostname"
#   excludeStable: false # Set to true if you want anti-affinity between canary and stable

#   # For high traffic scenarios
#   highTrafficMode:
#     enabled: false
#     weight: 50

# - enabled: true
#   weight: 80
#   excludeStable: false # Set to true if you want anti-affinity between canary and stable
#   topologyKey: "topology.kubernetes.io/zone"

#   # For high traffic scenarios
#   highTrafficMode:
#     enabled: false
#     weight: 30
podAffinity: []

# @schema
# additionalProperties: true
# @schema
# -- Topology spread constraints for pod distribution
# - enabled: true
#   maxSkew: 1
#   topologyKey: "topology.kubernetes.io/zone"
#   whenUnsatisfiable: "ScheduleAnyway"
#   # For high traffic scenarios
#   highTrafficMode:
#     enabled: false # Can be toggled via Helm values override
topologySpreadConstraints: []

# @schema
# type: boolean
# @schema
# -- Enable common topology spread constraints for automatic pod distribution
# When true, applies default constraints for zone and node spreading
useCommonTopologySpreadConstraints: false

# @schema
# additionalProperties: true
# @schema
# -- Environment variables for runtime configuration and JVM tuning
# -- JVM configuration for memory management, garbage collection, and security
# Memory settings: heap (128m initial and max)
# Security: disable log4j lookups to prevent log4shell vulnerability
envVars: {}

# @schema
# additionalProperties: true
# @schema
# -- Environment variables loaded from ConfigMaps for non-sensitive configuration
# Useful for feature flags, API endpoints, logging levels, and operational settings
# Example configuration:
# LOG_LEVEL: INFO                     # Application logging verbosity
# DEBUG_MODE: "false"                 # Enable debug features
# API_TIMEOUT: "30s"                  # External API timeout
# CACHE_TTL: "300"                    # Cache time-to-live in seconds
configMapEnvVars: {}

# @schema
# additionalProperties: true
# @schema
# -- ConfigMap files for application configuration, properties, and settings
# Files are mounted to /var/migrator/jetty/resources via projected volume (optional, read-only)
# Combined with secrets in a single mount point for unified configuration access
configMap: {}

# @schema
# additionalProperties: true
# @schema
# -- Environment variables loaded from Secrets for sensitive configuration
# Use for passwords, API keys, tokens, and other confidential data
# Example sensitive configuration:
# DATABASE_PASSWORD: postgres-password   # Database connection password
# API_TOKEN: external-service-token      # Third-party API authentication
# JWT_SECRET: signing-key                # Token signing secret
# ENCRYPTION_KEY: data-encryption-key    # Data encryption key
secretsEnvVars: {}

# @schema
# additionalProperties: true
# @schema
# -- Kubernetes Secret data for storing sensitive information securely
# Values are automatically base64 encoded and encrypted at rest
# Files are mounted to /var/migrator/jetty/resources via projected volume (optional, read-only)
# Combined with configMap in a single mount point for unified configuration access
# Example sensitive data:
# database-password: "supersecret123"    # Database credentials
# api-key: "sk-1234567890abcdef"         # External API key
# tls-cert: "-----BEGIN CERTIFICATE----" # TLS certificate
# signing-key: "private-key-content"     # JWT signing key
secrets: {}

# @schema
# additionalProperties: true
# @schema
# -- Argo Rollout configuration for advanced deployment strategies
rollout:

  # -- Whether to enable Argo Rollouts
  enabled: true

  # -- Number of old ReplicaSets to retain for rollback
  revisionHistoryLimit: 10

  # @schema
  # additionalProperties: true
  # @schema
  # -- Rollout strategy configuration
  strategy:

    # @schema
    # additionalProperties: true
    # @schema
    # -- Blue-Green deployment strategy
    blueGreen:

      # -- Whether to enable blue-green deployments
      enabled: false

      # -- Whether to automatically promote new versions
      autoPromotionEnabled: false

      # -- Seconds to wait before auto-promotion
      autoPromotionSeconds: 30

      # -- Number of preview replicas (null uses main replica count)
      previewReplicaCount:

      # -- Seconds to wait before scaling down old version
      scaleDownDelaySeconds: 30

    # @schema
    # additionalProperties: true
    # canary:
    # @schema
    # -- Canary deployment strategy
    canary:

      # -- Whether to enable canary deployments

      enabled: true

      # @schema
      # additionalProperties: true
      # trafficRouting:
      # @schema
      # -- Istio traffic routing configuration for canary
      trafficRouting:

        # @schema
        # additionalProperties: true
        # @schema
        istio:

          # @schema
          # additionalProperties: true
          # @schema
          virtualService:
            routes:
              - primary

          # @schema
          # additionalProperties: true
          # @schema
          destinationRule:
            canarySubsetName: canary
            stableSubsetName: stable

      # @schema
      # additionalProperties: true
      # @schema
      # -- Routes managed by the rollout controller
      managedRoutes: []

      # @schema
      # additionalProperties: true
      # @schema
      # -- Canary deployment steps with traffic weights and pauses
      steps:
        - setWeight: 1
        - pause: {}
        - setWeight: 5
        - pause: {}
        - setWeight: 10
        - pause:
            duration: 10m
        - setWeight: 20
        - pause:
            duration: 10m
        - setWeight: 30
        - pause:
            duration: 10m
        - setWeight: 40
        - pause:
            duration: 10m
        - setWeight: 50
        - pause:
            duration: 10m
        - setWeight: 60
        - pause:
            duration: 10m
        - setWeight: 80
        - pause:
            duration: 10m
        - setWeight: 100
        - pause: {}

      # -- Maximum Unavailable of surge pods during canary
      maxUnavailable: 0

      # -- Maximum number of surge pods during canary
      maxSurge: 1

  # @schema
  # additionalProperties: true
  # @schema
  # -- Analysis configuration for automated rollout decisions
  analysis:

    # -- Whether to enable rollout analysis
    enabled: false

    # -- Number of analysis runs to perform
    count: 3

    # -- Number of failures before rollback
    failureLimit: 1

    # -- Interval between analysis runs
    interval: 1m

    # -- Response time threshold in milliseconds
    responseTimeThreshold: 500

    # -- Step at which to start analysis
    startingStep: 2

    # -- Success condition threshold (0.99 = 99% success rate)
    successCondition: 0.99

    # -- Prometheus configuration for metrics collection
    prometheus:

      # -- Prometheus server address
      address: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090

    # -- Analysis templates to use
    templates:
      - templateName: success-rate

# @schema
# additionalProperties: false
# @schema
# -- Domain configuration for external access through Istio service mesh
# Note: When no hosts are configured (both public and private disabled or empty),
# Argo Rollouts will still work using mesh-internal traffic routing via the service's cluster.local DNS name
hosts:

  # @schema
  # additionalProperties: true
  # @schema
  # -- Public domains accessible from the internet for client connections
  # Enable this and configure domains when the service needs external ingress
  public:
    enabled: false

    # @schema
    # type: array
    # additionalProperties: true
    # @schema
    domains: []

  # @schema
  # additionalProperties: true
  # @schema
  # -- Internal/Private Domains not accessible from the internet for client connections
  # Enable this and configure domains when the service needs private ingress (VPN, internal network)
  private:
    enabled: false
    # @schema
    # type: array
    # additionalProperties: true
    # @schema
    # -- Internal/Private Domains not accessible from the internet for client connections
    domains: []

# @schema
# additionalProperties: true
# @schema
# -- Istio service mesh configuration
istio:

  # -- Whether to enable Istio service mesh
  enabled: true

  # -- Global Istio configurations
  globals:

    # -- Global annotations for Istio resources
    annotations: {}

  # -- Ambient Mode configuration for ztunnel and waypoint proxies
  ambient:

    # -- Whether to enable Ambient Mode (ztunnel mesh)
    enabled: true

    # -- Namespace labels for ambient mode enrollment
    namespaceLabels:
      istio.io/dataplane-mode: ambient

    # -- Waypoint proxy configuration for L7 features
    waypoint:

      # -- Whether to create a waypoint proxy for this service
      enabled: false

      # -- Traffic type for waypoint (service or workload)
      trafficType: service

  # -- Sidecar Mode configuration for envoy proxy injection
  sidecar:

    # -- Whether to enable Sidecar Mode (envoy proxy injection)
    enabled: false

    # -- Sidecar injection configuration
    injection:

      # -- Sidecar injection mode (auto, enabled, disabled)
      mode: auto

      # -- Pod-level sidecar injection annotation
      podAnnotation: sidecar.istio.io/inject

      # -- Namespace-level sidecar injection label
      namespaceLabel: istio-injection

    # -- Sidecar proxy configuration
    proxy:

      # -- Custom proxy configuration
      config: {}

      # -- Resource limits for sidecar proxy
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
        limits:
          cpu: 200m
          memory: 256Mi

      # -- Sidecar proxy image (uses Istio default if empty)
      image: ""

      # -- Sidecar proxy log level
      logLevel: warning

  # -- Certificate Manager integration
  certManager:

    # -- Whether to enable cert-manager for TLS certificates
    enabled: false

    # -- Certificate issuer to use
    issuer: "letsencrypt-prod"

  # -- Istio Gateway configuration
  gateway:

    # -- Whether to create an Istio Gateway
    create: true

    # -- Gateway name (generated if empty)
    name: ""

    # -- Gateway annotations
    annotations: {}

    # -- Gateway Labels
    labels:
      external-dns: "true"

    # -- Gateway selector
    selector:
      istio: "ingressgateway"

    # -- Private Gateway selector to exposure service for private traffic
    private:
      selector:
        app: "istio-private-gateway"

    # -- Public Gateway selector to exposure service for public traffic
    public:
      selector:
        app: "istio-public-gateway"

    # -- Gateway server configurations
    servers:

      # -- Whether this server configuration is enabled
      - enabled: true

        # -- Port configurations
        ports:
          - number: 80
            name: http
            protocol: HTTP

        # -- TLS configuration
        tls:

          # -- Whether TLS is enabled
          enabled: true

          # -- TLS configuration options
          config:

            # -- Whether to redirect HTTP to HTTPS
            httpsRedirect: true

      # -- Whether this server configuration is enabled
      - enabled: true

        # -- Port configurations for HTTPS
        ports:
          - number: 443
            name: https
            protocol: HTTP

        # -- TLS configuration for HTTPS
        tls:

          # -- Whether TLS is enabled
          enabled: true

          # -- TLS configuration options
          config:

            # -- TLS mode (AUTO_PASSTHROUGH, SIMPLE, etc.)
            mode: AUTO_PASSTHROUGH

      # -- Whether this server configuration is enabled
      - enabled: false

        # -- Port configurations
        ports:
          - number: 80
            name: tcpsocket
            protocol: HTTP

        # -- TLS configuration
        tls:

          # -- Whether TLS is enabled
          enabled: true

          # -- TLS configuration options
          config:
            # -- Whether to redirect HTTP to HTTPS
            httpsRedirect: true

    # -- Additional gateway configuration options
    additionalConfig: {}

  # -- Istio VirtualService configuration
  virtualService:

    # -- Host for the VirtualService (defaults to service name)
    host: ""

    # -- Gateways to attach to the VirtualService
    gateways: []

    # -- Additional route configuration
    additionalRouteConfig: {}

    # -- Additional HTTP routes
    additionalHttp: []

  # -- Istio DestinationRule configuration
  destinationRule:

    # -- Traffic policy for load balancing and connection pooling
    trafficPolicy:
      enabled: false
      config:
        connectionPool:
          tcp:
            maxConnections: 100
            connectTimeout: 30s
            tcpKeepalive:
              time: 7200s
              interval: 75s
          http:
            http1MaxPendingRequests: 64
            http2MaxRequests: 1000
            maxRequestsPerConnection: 10
            maxRetries: 3
        loadBalancer:
          simple: LEAST_CONN
        outlierDetection:
          consecutiveGatewayErrors: 5
          consecutive5xxErrors: 5
          interval: 30s
          baseEjectionTime: 30s
          maxEjectionPercent: 50

  # -- Fault injection configuration for testing
  faultInjection:

    # -- Whether to enable fault injection
    enabled: false

    # -- Fault injection configurations
    configs: {}

# @schema
# additionalProperties: true
# @schema
# -- ServiceMonitor configuration for Prometheus metrics collection
serviceMonitor:

  # -- Whether to enable ServiceMonitor for Prometheus scraping
  enabled: true

  # -- Namespace where ServiceMonitor should be deployed (if different from app namespace)
  namespace: ""

  # -- Prometheus release name for ServiceMonitor
  prometheusReleaseName: kube-prometheus-stack

  # -- Scraping interval for metrics
  interval: 10s

  # -- Scraping timeout for metrics (should be less than interval)
  scrapeTimeout: 5s

  # -- Metrics endpoint path
  path: /metrics

  # @schema
  # additionalProperties: true
  # labels:
  # @schema
  # -- Additional labels for ServiceMonitor
  labels: {}

  # -- Namespace selector configuration for cross-namespace monitoring
  namespaceSelector:

    # -- Enable any namespace selector (allows monitoring across namespaces)
    any: false

    # -- Specific namespaces to monitor (leave empty for current namespace only)
    matchNames: []

# @schema
# additionalProperties: false
# @schema
# -- Datadog configuration
datadog:

  # -- Enable/disable Datadog monitoring (disabled by default - enabled when annotations found)
  enabled: false

  # -- Container name (should match your main container name)
  containerName: "base-chart"

  # -- Legacy DataDog Configuration
  v1:

    # -- Enable/disable Datadog v1 monitoring (legacy, disabled by default - use v2 instead)
    enabled: false

    # -- Datadog Configurations
    configs:

      check_names: []

      init_configs: []

      instances: []

  # -- New DataDog Configuration
  v2:

    # -- Enable/disable Datadog monitoring
    enabled: false

    # -- Namespace prefix for metrics in Datadog
    namespace: "base-chart"

    # -- Metrics endpoint path
    metricsPath: "/metrics"

    # -- Collect all metrics (if true, ignores specific metrics list)
    collectAllMetrics: true

    # -- Timeout for metric collection
    timeout: 20

    # -- Enable health service check
    healthServiceCheck: true

    # -- Send histogram buckets
    sendHistogramsBuckets: true

    # -- Collect histogram buckets
    collectHistogramBuckets: true

    # -- Specific metrics to collect with transformations
    # Can be either strings (collect as-is) or objects (rename)
    metrics:

      # Collect these metrics as-is
      - "http_requests_total"
      - "http_request_duration_seconds"
      - "database_connections_active"
      - source: "go_memstats_alloc_bytes"
        target: "memory_allocated"
      - source: "process_cpu_seconds_total"
        target: "cpu_usage_seconds"
      - source: "base_custom_metric_total"
        target: "custom_operations"
      - source: "prometheus_rule_evaluation_duration_seconds"
        target: "rule_evaluation_time"

    # -- Metrics to ignore/exclude
    ignoreMetrics:
      - "go_gc_.*"
      - "go_goroutines"
      - "process_.*_bytes"
      - "prometheus_.*"
      - "up"
    metricTransformations:
      labels:
        - from: "handler"
          to: "endpoint"
        - from: "method"
          to: "http_method"
        - from: "status_code"
          to: "response_code"

    # -- Additional tags to add to all metrics
    # Note: version tag will be automatically replaced with the actual app version from Chart.yaml or image.tag
    tags:
      env: production
      team: backend
      service: base-chart
    logs:
      enabled: true
      source: "base-chart"
      service: "base-chart"

      # Log processing rules
      logProcessingRules:
        - type: "exclude_at_match"
          name: "exclude_health_checks"
          pattern: "GET /health"
        - type: "mask_sequences"
          name: "mask_credit_cards"
          pattern: "\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}"
          replacement: "****-****-****-****"

    # -- JMX (Java Management Extensions) integration configuration
    # Complete configuration for JMX monitoring with Datadog Agent
    jmx:

      # -- JMX connection URL (auto-generated if not provided)
      # Example: "service:jmx:rmi:///jndi/rmi://%%host%%:9016/jmxrmi"
      jmxUrl: ""

      # -- JMX authentication username
      user: ""

      # -- JMX authentication password (store in secret for production)
      password: ""

      # -- Whether this is a JMX integration (default: true when jmx integration is selected)
      isJmx: true

      # -- Collect default JMX metrics (heap, threads, GC, etc.)
      collectDefaultMetrics: true

      # -- Process name regex for JMX connection discovery
      # Example: ".*java.*base-chart.*"
      processNameRegex: ""

      # -- Custom tools.jar path for JMX tools
      tools_jar_path: ""

      # -- Custom name identifier for this JMX instance
      name: ""

      # -- Custom Java binary path
      java_bin_path: ""

      # -- JVM options for JMX connection
      # Example: "-Xms64m -Xmx128m"
      java_options: ""

      # -- SSL/TLS trust store path for secure JMX connections
      trust_store_path: ""

      # -- SSL/TLS trust store password
      trust_store_password: ""

      # -- SSL/TLS key store path for client certificates
      key_store_path: ""

      # -- SSL/TLS key store password
      key_store_password: ""

      # -- Enable SSL for RMI registry connections
      rmi_registry_ssl: false

      # -- RMI connection timeout in milliseconds
      rmi_connection_timeout: 20000

      # -- RMI client timeout in milliseconds
      rmi_client_timeout: 15000

      # -- Collect default JVM metrics (memory, GC, threads)
      collect_default_jvm_metrics: true

      # -- Enable new garbage collection metrics format
      new_gc_metrics: true

      # -- Custom service check prefix for JMX health checks
      service_check_prefix: "jmx"

      # -- Custom metric configuration for JMX beans
      # Define specific MBeans and attributes to collect
      conf:
        - include:
            domain: kafka.consumer
            bean_regex: kafka.consumer:type=consumer-fetch-manager-metrics,client-id=.*
            attribute:
              records-consumed-rate:
                metric_type: gauge
                alias: kafka.consumer.messages_in_rate
              records-lag-max:
                metric_type: gauge
                alias: kafka.consumer.max_lag
              bytes-consumed-rate:
                metric_type: gauge
                alias: kafka.consumer.bytes_in_rate
        - include:
            domain: kafka.consumer
            bean_regex: kafka.consumer:type=consumer-coordinator-metrics,client-id=.*
            attribute:
              assigned-partitions:
                metric_type: gauge
                alias: kafka.consumer.num_partitions
              heartbeat-rate:
                metric_type: gauge
                alias: kafka.consumer.heartbeat
              commit-rate:
                metric_type: gauge
                alias: kafka.consumer.commit_rate
      host: '%%host%%'
      port: 9016

    # -- Custom integration configuration
    # For integrations other than openmetrics or jmx
    instanceConfig: {}

    # Example custom integration:
    # host: "%%host%%"
    # port: 8080
    # username: "monitor"
    # password: "secret"
    # ssl: true
    # timeout: 30

    # -- Integration type: openmetrics, jmx, or custom
    # openmetrics: For Prometheus-style metrics endpoints
    # jmx: For Java Management Extensions monitoring
    # custom: For other monitoring integrations
    integration: kafka

    # -- Custom init configuration for the integration
    # Additional configuration passed to the integration's init_config
    initConfig: {}

    # -- APM (Application Performance Monitoring) configuration
    apm:
      enabled: false
      serviceName: "base-chart"
      environment: "production"

# @schema
# additionalProperties: true
# @schema
# -- NetworkPolicy configuration for defense-in-depth network security
# Implements zero-trust networking with explicit allow rules for required communication
networkPolicy:
  # -- Enable NetworkPolicy creation for enhanced network security
  enabled: false
  # -- Deny-all baseline policy configuration
  denyAll:
    # -- Create a deny-all NetworkPolicy as security baseline (recommended for production)
    enabled: false
  # -- Ingress traffic rules configuration
  ingress:
    # -- Allow ingress from pods within the same namespace
    allowSameNamespace: true
    # -- Allow ingress from Istio Gateway for service mesh traffic
    allowIstioGateway: true
    # -- Allow ingress from monitoring namespace for metrics collection
    allowMonitoring: true
    # -- Name of the monitoring namespace (default: monitoring)
    monitoringNamespace: "monitoring"
    # -- Custom ingress rules for specific traffic requirements
    # Example custom rules:
    # - namespaceSelector:
    #     matchLabels:
    #       name: "api-gateway"
    #   ports:
    #     - protocol: TCP
    #       port: 8080
    # - ipBlock:
    #     cidr: "10.0.0.0/8"
    #   ports:
    #     - protocol: TCP
    #       port: 8080
    customRules: []
  # -- Egress traffic rules configuration
  egress:
    # -- Enable egress policies (when false, all egress is allowed)
    enabled: false
    # -- Allow DNS resolution (required for service discovery)
    allowDNS: true
    # -- Allow HTTPS traffic to internet (for external APIs, image pulls)
    allowHTTPS: true
    # -- Allow traffic within the same namespace
    allowSameNamespace: true
    # -- Allow access to AWS metadata service (required for AWS integrations)
    allowAWSMetadata: true
    # -- Custom egress rules for specific external services
    # Example custom rules:
    # - namespaceSelector:
    #     matchLabels:
    #       name: "database"
    #   ports:
    #     - protocol: TCP
    #       port: 5432
    # - ipBlock:
    #     cidr: "192.168.1.0/24"
    #   ports:
    #     - protocol: TCP
    #       port: 443
    customRules: []
  # -- Monitoring-specific NetworkPolicy configuration
  monitoring:
    # -- Create dedicated NetworkPolicy for monitoring access
    enabled: true
    # -- Prometheus namespace for metrics scraping
    prometheusNamespace: "monitoring"
    # -- Datadog namespace for agent access
    datadogNamespace: "base-chart-datadog"

# @schema
# additionalProperties: true
# @schema
# -- Pod Security Standards configuration for compliance automation
# Implements Kubernetes Pod Security Standards for enhanced security posture
podSecurityStandards:

  # -- Enable Pod Security Standards compliance features
  enabled: true

  # -- Pod Security Standards level: privileged, baseline, or restricted
  # restricted: Most secure, suitable for security-critical applications
  # baseline: Minimally restrictive, prevents known privilege escalations
  # privileged: Unrestricted, suitable for system-level workloads
  level: "restricted"

  # -- Create PodSecurityPolicy for clusters without Pod Security Standards
  # (Kubernetes < 1.25 or clusters still using PSP)
  createPodSecurityPolicy: false

  # -- Create SecurityContextConstraints for OpenShift clusters
  createSecurityContextConstraints: false

  # -- Allow hostPath volumes (not recommended for restricted level)
  allowHostPaths: false

  # -- Allowed hostPath configurations (only if allowHostPaths is true)
  # Example:
  # - pathPrefix: "/tmp"
  #   readOnly: true
  # - pathPrefix: "/var/log"
  #   readOnly: false
  allowedHostPaths: []

  # -- Allowed host port ranges (empty means no host ports allowed)
  # Example:
  # - min: 8000
  #   max: 8080
  allowedHostPortRanges: []

  # -- Allowed Linux capabilities (empty for restricted level)
  # Example (not recommended for restricted):
  # - "NET_BIND_SERVICE"
  # - "SYS_TIME"
  allowedCapabilities: []

  # -- Allowed seccomp profiles beyond RuntimeDefault
  # Example:
  # - type: "Localhost"
  #   localhostProfile: "my-profile.json"
  allowedSeccompProfiles: []

  # -- AppArmor configuration
  apparmor:

    # -- Enable AppArmor profiles
    enabled: false

    # -- AppArmor profiles for containers
    # Example:
    # base-chart: "runtime/default"
    # sidecar: "localhost/my-profile"
    profiles: {}

  # -- Compliance and audit settings
  compliance:

    # -- Enable compliance monitoring and reporting
    enabled: true

    # -- Compliance frameworks to validate against
    frameworks:
      - "pod-security-standards"
      - "cis-kubernetes"
      - "nist-cybersecurity"
    auditLogging:
      enabled: true

      # -- Violation severity levels to log
      levels:
        - "warn"
        - "error"

# @schema
# additionalProperties: true
# @schema
# -- Backup automation configuration for operational excellence
# Provides multiple backup strategies including Velero and custom CronJob backups
backup:
  # -- Enable backup automation features
  enabled: false
  # -- Velero backup configuration for full cluster backup
  velero:
    # -- Enable Velero backup scheduling
    enabled: false
    # -- Backup schedule in cron format (default: daily at 2 AM)
    schedule: "0 2 * * *"
    # -- Backup retention period (default: 30 days)
    retention: "720h"
    # -- Velero namespace (default: velero)
    namespace: "velero"
    # -- Storage location for backups
    storageLocation: "default"
    # -- Volume snapshot locations for persistent volumes
    volumeSnapshotLocations:
      - "default"
    includeCustomResources: true
    # -- Custom resources to include in backup
    customResources:
      - "rollouts.argoproj.io"
      - "virtualservices.networking.istio.io"
      - "destinationrules.networking.istio.io"
      - "gateways.networking.istio.io"
    hooks:
      enabled: true
  # -- Custom CronJob backup for application-specific data
  cronjob:
    # -- Enable CronJob-based backup
    enabled: false
    # -- Backup schedule in cron format (default: daily at 3 AM)
    schedule: "0 3 * * *"
    # -- Timezone for backup schedule
    timeZone: "UTC"
    # -- Concurrency policy for backup jobs
    concurrencyPolicy: "Forbid"
    # -- Number of successful backup jobs to retain
    successfulJobsHistoryLimit: 3
    # -- Number of failed backup jobs to retain
    failedJobsHistoryLimit: 1
    # -- Deadline for backup job to start
    startingDeadlineSeconds: 300
    # -- Number of retries for failed backup jobs
    backoffLimit: 2
    # -- Maximum time for backup job to complete
    activeDeadlineSeconds: 3600
    # -- Retention period for backups (in days)
    retentionDays: 30
    # -- Type of backup to perform
    backupType: "application-data"
    # -- Container image for backup executor
    image:
      repository: "amazon/aws-cli"
      tag: "2.13.0"
      pullPolicy: "IfNotPresent"
    # -- Resource limits and requests for backup job
    resources:
      limits:
        cpu: "500m"
        memory: "512Mi"
      requests:
        cpu: "100m"
        memory: "128Mi"

    # -- S3 configuration for backup storage
    s3:

      # -- S3 bucket for backup storage
      bucket: ""

      # -- S3 prefix for backup objects
      prefix: "backups"

    # -- Additional environment variables for backup job
    # Example:
    # AWS_REGION: "us-east-1"
    # BACKUP_ENCRYPTION: "true"
    extraEnv: {}

    # -- Additional volumes for backup job
    # Example:
    # - name: config
    #   configMap:
    #     name: backup-config
    volumes: []

    # -- Additional volume mounts for backup job
    # Example:
    # - name: config
    #   mountPath: /etc/backup
    #   readOnly: true
    volumeMounts: []

    # -- Node selector for backup job placement
    nodeSelector: {}
    # Example:
    # kubernetes.io/arch: amd64
    # node-role.kubernetes.io/worker: "true"
    # -- Tolerations for backup job scheduling
    # Example:
    # - key: "backup-node"
    #   operator: "Equal"
    #   value: "true"
    #   effect: "NoSchedule"
    tolerations: []

  # -- Monitoring configuration for backup operations
  monitoring:
    # -- Enable backup monitoring and metrics
    enabled: true
    # -- Prometheus metrics for backup status
    metrics:
      # -- Enable backup metrics collection
      enabled: true
      # -- Metrics endpoint path
      path: "/metrics"
      # -- Metrics collection interval
      interval: "30s"
    # -- Alerting configuration for backup failures
    alerting:
      # -- Enable backup failure alerts
      enabled: true
      # -- Alert severity levels
      severity: "warning"
      # -- Alert notification channels
      channels:
        - "slack"
        - "email"

# @schema
# additionalProperties: true
# @schema
# -- Grafana Dashboards Configuration
grafana:
  # -- Dashboard configuration
  dashboards:
    # -- Enable Grafana dashboard ConfigMap creation
    # Only creates ConfigMap if dashboard file exists at dashboards/base-chart.json
    enabled: true
    # -- Grafana folder for dashboard organization
    folder: "Viafoura"
    # -- Additional labels for dashboard ConfigMap discovery
    labels:
      grafanaDashboard: "1"
      dashboardSource: "base-chart"
    # @schema
    # additionalProperties: true
    # type: object
    # @schema
    # -- Grafana Dashboard Annotations
    # Example annotations:
    # grafana.com/dashboard-uid: "base-chart-dashboard"
    # grafana.com/dashboard-title: "Base-Chart Service Monitoring"
    annotations:
      grafana.com/dashboard-uid: "base-chart-dashboard"
      grafana.com/dashboard-title: "Base-Chart Service Monitoring"

placement:
  affinityMode: none
  nodeSelector: {}
  tolerations: []

system:
  enablePtrace: false

# @schema
# additionalProperties: true
# @schema
# -- Prometheus alerting rules configuration
alerts:
  # -- Enable Prometheus alerting rules
  enabled: false
  # -- How often to evaluate alert rules
  evaluationInterval: 30s
  # -- Dashboard URL for alert annotations
  dashboardUrl: https://grafana.example.com/d/base-chart
  # -- Runbook URL base for alert documentation
  runbookUrl: https://wiki.example.com/runbooks/base-chart
  # -- Additional labels for PrometheusRule
  labels: {}

  # -- Alert rule configurations
  rules:
    # High WebSocket connections per pod
    highConnectionsPerPod:
      enabled: true
      threshold: 45000
      duration: 2m
      severity: warning

    # High GC pause time
    highGCPauseTime:
      enabled: true
      threshold: 200  # milliseconds
      duration: 5m
      severity: warning

    # Pod restart rate
    podRestarts:
      enabled: true
      threshold: 2  # restarts per hour
      duration: 5m
      severity: critical

    # Message processing latency (P99)
    messageLatency:
      enabled: true
      threshold: 500  # milliseconds
      duration: 5m
      severity: warning

    # Memory usage percentage
    podMemory:
      enabled: true
      threshold: 90  # percentage
      duration: 5m
      severity: warning

    # Connection drop rate
    connectionDropRate:
      enabled: true
      threshold: 10  # drops per second
      duration: 5m
      severity: warning

    # Service availability
    serviceDown:
      enabled: true
      duration: 2m
      severity: critical

  # -- Custom alert rules
  # Example:
  # - name: CustomAlert
  #   expr: 'some_metric > 100'
  #   for: 5m
  #   severity: warning
  #   labels:
  #     team: backend
  #   annotations:
  #     summary: "Custom alert triggered"
  #     description: "Custom metric exceeded threshold"
  customRules: []
